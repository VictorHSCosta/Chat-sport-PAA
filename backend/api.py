# -*- coding: utf-8 -*-
"""
API FastAPI com LlamaIndex para Chat sobre Copa do Mundo FIFA
Vers√£o Essencial - Groq (llama3-70b-8192) + Dataset CSV
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import pandas as pd
import os
import uvicorn
from typing import Optional
from dotenv import load_dotenv

# Carregar vari√°veis de ambiente do arquivo .env
load_dotenv()

# LlamaIndex imports
from llama_index.llms.groq import Groq

class QuestionRequest(BaseModel):
    message: str

class QuestionResponse(BaseModel):
    answer: str
    success: bool
    message: str = ""

class HealthResponse(BaseModel):
    status: str
    message: str

app = FastAPI(
    title="World Cup RAG API with LlamaIndex",
    description="API para chatbot sobre Copa do Mundo FIFA usando LlamaIndex + Groq",
    version="4.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://127.0.0.1:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Vari√°veis globais
llm = None
template = None
datasets_loaded = False

# Configura√ß√£o da API Key do Groq
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

# Provedor LLM - usando apenas Groq para simplicidade
LLM_PROVIDER = "groq"

def initialize_llm():
    """Inicializar o LLM Groq"""
    global llm
    try:
        if not GROQ_API_KEY:
            print("‚ùå GROQ_API_KEY n√£o configurada!")
            print("üí° Configure com: export GROQ_API_KEY=sua_chave_aqui")
            return False
            
        print("üöÄ Inicializando Groq LLM...")
        llm = Groq(model='llama3-70b-8192', api_key=GROQ_API_KEY)
        print("‚úÖ Groq LLM inicializado com sucesso!")
        return True
    except Exception as e:
        print(f"‚ùå Erro ao inicializar Groq LLM: {e}")
        return False

def load_csv_datasets():
    """Carregar os datasets CSV essenciais e criar o template"""
    global template, datasets_loaded
    
    try:
        print("üìä Carregando datasets CSV...")
        
        # Apenas datasets essenciais (removido ranking para simplificar)
        csv_paths = [
            "wcdataset/world_cup.csv",
            "wcdataset/matches_1930_2022.csv"
        ]
        
        dataframes = []
        loaded_files = []
        
        for csv_path in csv_paths:
            if os.path.exists(csv_path):
                try:
                    df = pd.read_csv(csv_path, encoding='latin1')
                    print(f"‚úÖ {csv_path} carregado - {len(df)} linhas")
                    dataframes.append(df)
                    loaded_files.append(csv_path)
                except Exception as e:
                    print(f"‚ö†Ô∏è Erro ao carregar {csv_path}: {e}")
            else:
                print(f"‚ö†Ô∏è Arquivo n√£o encontrado: {csv_path}")
        
        if not dataframes:
            print("‚ùå Nenhum dataset foi carregado!")
            return False
        
        # Otimizar datasets para o Groq (limite de tokens)
        optimized_dataframes = []
        optimized_files = []
        
        for i, df in enumerate(dataframes):
            if "world_cup" in loaded_files[i]:
                # Manter world_cup.csv COMPLETO - √© o dataset mais importante
                print(f"‚úÖ Dataset {loaded_files[i]} mantido COMPLETO com {len(df)} linhas")
            elif "matches" in loaded_files[i]:
                # Matches: apenas 5 jogos mais importantes para economizar tokens
                df = df.head(5)
                print(f"‚öΩ Dataset {loaded_files[i]} otimizado para {len(df)} linhas")
            
            optimized_dataframes.append(df)
            optimized_files.append(loaded_files[i])
        
        dataframes = optimized_dataframes
        loaded_files = optimized_files
        
        # Criar template ultra-compacto para Groq
        template = "Copa do Mundo FIFA - Dados Oficiais:\n"

        # Adicionar dados de forma ultra-compacta
        for i, df in enumerate(dataframes):
            file_name = loaded_files[i].split('/')[-1]
            print(f"üìÑ Processando {file_name}...")
            
            if "world_cup" in file_name:
                # World Cup: formato ultra-compacto - apenas essencial
                template += "COPAS:\n"
                for _, row in df.iterrows():
                    # Apenas dados cr√≠ticos: Ano|Sede|Campe√£o|Vice|Artilheiro
                    template += f"{row['Year']}|{row['Host']}|{row['Champion']}|{row['Runner-Up']}|{row['TopScorrer']}\n"
                    
            elif "matches" in file_name:
                # Matches: ultra limitado - apenas 3 jogos mais importantes
                template += "JOGOS:\n"
                for _, row in df.head(3).iterrows():
                    # Dados essenciais dos jogos
                    cols = df.columns.tolist()[:4]  # Apenas primeiras 4 colunas
                    values = [str(row[col]) if pd.notna(row[col]) else "-" for col in cols]
                    template += "|".join(values) + "\n"
        
        print(f"‚úÖ Template criado com {len(template)} caracteres")
        datasets_loaded = True
        return True
        
    except Exception as e:
        print(f"‚ùå Erro ao carregar datasets: {e}")
        return False

def initialize_system():
    """Inicializar todo o sistema"""
    print("üöÄ Inicializando sistema LlamaIndex + Groq...")
    
    # 1. Inicializar LLM Groq
    if not initialize_llm():
        return False
    
    # 2. Carregar datasets
    if not load_csv_datasets():
        return False
    
    print("üéâ Sistema inicializado com sucesso!")
    return True

def get_quick_answer(message: str) -> Optional[str]:
    """Respostas r√°pidas para perguntas comuns"""
    message_lower = message.lower().strip()
    
    quick_answers = {
        "oi": "Ol√°! Sou seu assistente especializado em Copa do Mundo FIFA. Como posso ajudar?",
        "ol√°": "Ol√°! Sou seu assistente especializado em Copa do Mundo FIFA. Como posso ajudar?",
        "hello": "Hello! I'm your FIFA World Cup specialist assistant. How can I help?",
        "hi": "Oi! Sou especialista em Copa do Mundo FIFA. O que voc√™ gostaria de saber?",
        
        # Respostas r√°pidas espec√≠ficas
        "quem ganhou a copa de 1970": "O Brasil conquistou a Copa do Mundo de 1970, realizada no M√©xico. Foi o terceiro t√≠tulo mundial brasileiro.",
        "artilheiro copa 1970": "Gerd M√ºller (Alemanha Ocidental) foi o artilheiro da Copa de 1970 com 10 gols.",
        "quantas copas o brasil tem": "O Brasil conquistou 5 Copas do Mundo: 1958 (Su√©cia), 1962 (Chile), 1970 (M√©xico), 1994 (EUA), 2002 (Jap√£o/Coreia do Sul).",
        "pr√≥xima copa": "A pr√≥xima Copa do Mundo ser√° em 2026, realizada conjuntamente pelos EUA, Canad√° e M√©xico, com 48 sele√ß√µes.",
    }
    
    return quick_answers.get(message_lower)

def process_question_with_llm(question: str) -> str:
    """Processar pergunta usando o LLM Groq com o template completo"""
    try:
        # Combinar template com a pergunta
        full_prompt = template + f"\n\nPERGUNTA: {question}\n\nResponda em portugu√™s baseado APENAS nos dados fornecidos acima:"
        
        # Usar o LLM Groq diretamente
        response = llm.complete(full_prompt)
        
        return response.text.strip()
        
    except Exception as e:
        print(f"‚ùå Erro ao processar pergunta: {e}")
        raise HTTPException(status_code=500, detail=f"Erro interno: {str(e)}")

@app.on_event("startup")
async def startup_event():
    """Evento de inicializa√ß√£o da API"""
    success = initialize_system()
    if not success:
        print("‚ùå AVISO: Sistema n√£o foi inicializado corretamente")

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Verificar sa√∫de da API"""
    if llm is None or not datasets_loaded:
        return HealthResponse(
            status="unhealthy",
            message="Sistema n√£o inicializado completamente"
        )
    return HealthResponse(
        status="healthy",
        message="API funcionando corretamente com LlamaIndex + Groq"
    )

@app.post("/chat", response_model=QuestionResponse)
async def ask_question(request: QuestionRequest):
    """Endpoint principal para fazer perguntas"""
    
    if llm is None or not datasets_loaded:
        raise HTTPException(status_code=500, detail="Sistema n√£o inicializado")
    
    if not request.message.strip():
        raise HTTPException(status_code=400, detail="Pergunta n√£o pode estar vazia")
    
    # Verificar respostas r√°pidas primeiro
    quick_answer = get_quick_answer(request.message)
    if quick_answer:
        return QuestionResponse(
            answer=quick_answer,
            success=True,
            message="Resposta r√°pida"
        )
    
    try:
        # Processar com LLM usando template completo
        answer = process_question_with_llm(request.message)
        
        return QuestionResponse(
            answer=answer,
            success=True,
            message="Resposta processada com LlamaIndex + Groq"
        )
        
    except Exception as e:
        print(f"‚ùå Erro no endpoint /chat: {e}")
        raise HTTPException(status_code=500, detail=f"Erro interno: {str(e)}")

@app.get("/")
async def root():
    """Endpoint raiz da API"""
    return {
        "message": "World Cup RAG API with LlamaIndex",
        "version": "4.0.0",
        "description": "Chatbot especializado em Copa do Mundo FIFA usando LlamaIndex + Groq",
        "llm_model": "llama3-70b-8192",
        "provider": "GROQ",
        "framework": "LlamaIndex",
        "endpoints": {
            "health": "/health",
            "chat": "/chat",
            "status": "/status",
            "docs": "/docs"
        }
    }

@app.get("/status")
async def status_check():
    """Status detalhado do sistema"""
    return {
        "system": "World Cup RAG API with LlamaIndex",
        "llm_ready": llm is not None,
        "datasets_loaded": datasets_loaded,
        "template_size": len(template) if template else 0,
        "model_config": {
            "llm_provider": "GROQ",
            "llm_model": "llama3-70b-8192",
            "framework": "LlamaIndex",
            "approach": "Direct CSV Template Injection",
            "language": "Portuguese",
            "anti_hallucination": True,
            "data_only": True
        },
        "data_sources": {
            "world_cup": "‚úÖ" if os.path.exists("wcdataset/world_cup.csv") else "‚ùå",
            "matches": "‚úÖ" if os.path.exists("wcdataset/matches_1930_2022.csv") else "‚ùå"
        }
    }

if __name__ == "__main__":
    uvicorn.run("api:app", host="0.0.0.0", port=8000, reload=True)
